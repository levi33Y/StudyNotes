**开场**



**功能介绍**（操作展示）

输入文本生成动作



**专业场景**（官网素材）





**整体架构**（架构图*2）

你的指令先经大语言模型优化和时长预测，再编码成语义特征，输入核心生成器。

核心的DiT模型采用混合注意力机制，先在双流块实现文本-动作语义交互，再在单流块深度融合。



**结尾**





我打算做一个关于腾讯混元的HY-Motion 1.0模型的分享视频，时长控制在60秒左右，内容和视频大致提纲素材我已经拟定好，如下描述，请根据下面描述帮我写好视频的脚本。



视频内容描述：官网文章头部展示 -- 我自己用提示词生成的操作展示 -- 官网中各类动作的例子展示 -- 框架概述和模型架构两张图依次展示



大致提纲如下：

【开头】：近日，腾讯混元发布了HY-Motion 1.0模型，该项目基于流匹配的 DiT 架构成功扩展至 1B+ 参数量级，通过扩大模型容量与数据规模，显著提升了动作生成的质量上限与泛化能力。

【操作展示】仅需一句自然语言描述，能生成高保真、流畅多样的 3D 角色骨骼动画。

【动作类型】生成动画涵盖基础移动、体育、户外、日常、社交活动和游戏角色动作，甚至可以实现组合动作和动作重定向

【架构解读】你的指令先经大语言模型优化和时长预测，再编码成语义特征，输入核心生成器。核心的DiT模型采用混合注意力机制，先在双流块实现文本-动作语义交互，再在单流块深度融合。

A person running and then sliding.





近日，腾讯混元发布 HY-Motion 1.0 模型。该项目基于流匹配的 DiT 架构，成功将参数扩展至 10亿 量级，通过扩大模型容量与数据规模，显著提升了动作生成的质量上限与泛化能力。



仅需输入一句语言描述，即可生成高保真、流畅的 3D 角色骨骼动作。



不仅覆盖体育、社交日常和游戏招式等多种场景，更能实现复杂的组合动作与动作重定向。



框架流程方面，用户指令首先经过大语言模型进行优化和时长预测，随后编码为语义特征，作为核心生成器的输入条件。



核心 DiT 模型采用混合注意力机制：首先在“双流块”中实现文本与动作的语义交互，随后进入“单流块”进行深度融合，以确保动作生成的准确性。



HY-Motion 1.0 为3D 动作生成技术确立和提供了新的范式与技术参考。

